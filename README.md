This document presents a GitHub-style README for the research paper "Multi-frame-based Dynamic Scene Lightweight Self-Supervised Monocular Depth Estimation," incorporating key images and citing the provided source information.MDL-Depth: Multi-frame-based Dynamic Scene Lightweight Self-Supervised Monocular Depth Estimation<div align="center"><img src="assets/MDL_Depth_Framework.png" alt="MDL-Depth Overall Framework" width="800"/><strong>Figure 2: The overall structural framework of the proposed lightweight depth estimation method MDL-Depth.</strong> 1</div><p align="center"><a href="https://arxiv.org/abs/XXXXXX">üìÑ Paper (arXiv)</a> |<a href="https://github.com/CdpLab/XXXXXX">üíª Code (GitHub)</a> |<a href="https://doi.org/XXXXXX">üîó DOI</a></p>üåü IntroductionThis paper proposes a unified lightweight single-frame and multi-frame fusion architecture named MDL-Depth to address the severe performance degradation of self-supervised monocular depth estimation in real-world dynamic scenes2222. Traditional methods suffer because they violate the "static world assumption" when independently moving objects, such as vehicles and pedestrians, are present3333.MDL-Depth is designed to mitigate interference from moving objects at both the data and loss levels, providing an effective solution for efficient and precise depth estimation in complex dynamic environments4444.üí° Key InnovationsThe main innovations of the MDL-Depth framework are summarized in three points5:Lightweight Unified Framework66:Proposes a dual-branch architecture with shared weights that collaboratively optimizes single-frame and multi-frame depth estimation7777.Leverages cross-branch knowledge distillation from the multi-frame branch (teacher) to the single-frame branch (student) to enhance the single-frame model's accuracy without increasing its inference costs8888.Explicit Motion Compensation Module999:Incorporates an optical flow network (GMflow 101010) to precisely capture pixel-level motion111111111111111111.Performs explicit feature warping operations to align reference frame features containing dynamic objects to the target frame's viewpoint, resolving motion misalignment issues at the data level prior to feature fusion121212121212121212.Motion-Aware Loss Function13:Develops an adaptive loss masking strategy tailored for dynamic foregrounds14.Analyzes reprojection errors between consecutive frames to generate dynamic masks ($M_{dyn}$) that actively identify and filter out erroneous gradient signals caused by motion mismatches, thereby improving reconstruction accuracy for both moving objects and static backgrounds15151515.<div align="center"><img src="assets/Dynamic_Masking_Principle.png" alt="Dynamic Masking Principle" width="600"/><strong>Figure 6: Schematic of Dynamic Masking Principle.</strong> 16</div>üìä Quantitative ResultsExperiments on the challenging KITTI and Cityscapes datasets demonstrate that MDL-Depth achieves high accuracy while maintaining an ultra-lightweight design171717171717171717.KITTI Dataset Comparison (640x192)MethodTest FramesParams(M) ‚ÜìAbsRel ‚ÜìSqRel ‚ÜìRMSE ‚ÜìRMSElog ‚ÜìŒ¥1‚Äã ‚ÜëŒ¥2‚Äã ‚ÜëŒ¥3‚Äã ‚ÜëLite-Mono 181 193.1 200.107 210.765 224.461 230.183 240.886 250.963 260.983 27MDL-Depth(Single)1 283.0 290.098 300.646 314.235 320.174 330.904 340.968 350.985 36Manydepth2 372 38- 390.091 400.649 414.232 420.170 430.909 440.968 450.985 46MDL-Depth(Multi)3 475.8 480.091 490.620 504.126 510.168 520.906 530.969 540.985 55Table 1 excerpt showing key comparison results on the KITTI dataset. 56565656Efficiency and Speed EvaluationMethodTestParameters(M) ‚ÜìFLOPs (G) ‚ÜìSpeed(ms) ‚ÜìAbsRel ‚ÜìLite-Mono 57Single-frame 583.1 595.1 603.2 610.107 62MDL-Depth(Single)Single-frame 633.0 645.0 652.8 660.098 67ManyDepth 68Multiframe 6926.9 7013.7 7115.1 720.098 73MDL-Depth(Multi)Multiframe 745.8 7512.3 7610.8 770.091 78Table 6 excerpt highlighting the parameter count, computational complexity (FLOPs), and inference speed. 79797979üñºÔ∏è Qualitative ResultsThe visualization results show MDL-Depth's superiority in generating depth maps with clearer details and more accurate geometric structures, especially when handling slender structures and moving objects80808080.KITTI Visualization<div align="center"><img src="assets/KITTI_Visualization.png" alt="KITTI Visualization Comparison" width="800"/><strong>Figure 7: Visualization comparing our model with Lite-mono and SC-depthv3 on the KITTI dataset.</strong> 81</div>Ablation Study VisualizationThe ablation study visually confirms the necessity of each component, with the complete model (A) most accurately restoring clear contours and precise depth for both dynamic and static objects82.<div align="center"><img src="assets/Ablation_Visualization.png" alt="Ablation Study Visualization" width="800"/><strong>Figure 10: Visual Comparison of Ablation Experiments.</strong> 83</div>üöÄ Getting StartedInstructions for setting up the environment, preparing datasets, training, and evaluation will be provided in the official repository.PrerequisitesPython 3.xPyTorch (The paper's implementation used PyTorch, but specific versions are not detailed in the abstract/introduction)Other dependencies for computer vision tasks.Training (Example structure)Bash# Training with ResNet18 backbone (KITTI, 640x192)
# The framework uses a shared-weight dual-branch architecture.

# Multi-GPU training:
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 \
  train.py --config configs/ResNet18_KITTI_MR.yaml
Evaluation (Example structure)Bash# Evaluate the single-frame model performance
python evaluate_depth.py \
  --pretrained_path path/to/MDL-Depth_Single.pth \
  --backbone LightweightEncoder \
  --split eigen

# Evaluate the multi-frame model performance
python evaluate_depth_mf.py \
  --pretrained_path path/to/MDL-Depth_Multi.pth \
  --backbone LightweightEncoder \
  --mdl_scale small \
  --split eigen
üìù CitationIf you find this work useful in your research, please consider citing our paper:‰ª£Á†ÅÊÆµ@article{liu2025multi,
  title={Multi-frame-based Dynamic Scene Lightweight Self-Supervised Monocular Depth Estimation},
  author={Liu, Jia and Lu, Guorui and Wang, Yiyang and Wei, Lina and Chen, Dapeng},
  journal={School of Automation, C-IMER, CICAEET, Nanjing University of Information Science and Technology, Nanjing, 210044, China},
  year={2025},
  note={Preprint submitted to Elsevier}
}
